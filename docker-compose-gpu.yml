version: '3.8'
services:
  backend:
    volumes:
      - "./:/app"
    build:
      context: .
      dockerfile: Dockerfile-gpu 
    depends_on:
      - super__redis
      - super__postgres
    networks:
      - super_network
    command: ["/app/wait-for-it.sh", "super__postgres:5432","-t","60","--","/app/entrypoint.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  celery:
    volumes:
      - "./:/app"
      - "${EXTERNAL_RESOURCE_DIR:-./workspace}:/app/ext"
    build:
      context: .
      dockerfile: Dockerfile-gpu 
    depends_on:
      - super__redis
      - super__postgres
    networks:
      - super_network
    command: ["/app/entrypoint_celery.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  gui:
    build:
      context: ./gui
      args:
        NEXT_PUBLIC_API_BASE_URL: "/api"
    networks:
      - super_network

  super__redis:
    image: "redis/redis-stack-server:latest"
    networks:
      - super_network
    volumes:
      - redis_data:/data

  super__postgres:
    image: "docker.io/library/postgres:15"
    environment:
      - POSTGRES_USER=superagi
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=super_agi_main
    volumes:
      - superagi_postgres_data:/var/lib/postgresql/data/
    networks:
      - super_network

  proxy:
    image: nginx:stable-alpine
    ports:
      - "3000:80"
    networks:
      - super_network
    depends_on:
      - backend
      - gui
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf

  ollama:
    image: ollama/ollama
    command: []
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - super_network

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    command: ["--port", "8002", "--num_workers", "8", "--model", "ollama/nous-hermes2-mixtral", "--api_base", "http://ollama:11434" ]
    depends_on:
      - ollama
    networks:
      - super_network

networks:
  super_network:
    driver: bridge

volumes:
  superagi_postgres_data:
  redis_data:
  ollama:
